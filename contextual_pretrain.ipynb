{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a61de0b7-e533-48d9-8d6f-ef62c5bd0c79",
      "metadata": {
        "id": "a61de0b7-e533-48d9-8d6f-ef62c5bd0c79"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QZ1jIrT5yMAd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ1jIrT5yMAd",
        "outputId": "7e78bb5e-15ef-4a2d-c9a5-e9e27422e7cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "evO0OutljQS2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evO0OutljQS2",
        "outputId": "f4ef6904-d820-4de8-def4-1a1077e1a716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec  2 01:41:10 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af90b400-b46b-4840-9392-3ce1c703bc20",
      "metadata": {
        "id": "af90b400-b46b-4840-9392-3ce1c703bc20"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "num_actions_train = 2500  # Number of actions/articles in training dataset\n",
        "num_actions_val = 1000    # Number of actions/articles in validation dataset\n",
        "T_train = 500             # Number of time periods for outcomes per action in training\n",
        "T_val = 500               # Number of time periods for outcomes per action in validation\n",
        "\n",
        "# Data generation\n",
        "np.random.seed(42)  # Set random seed for reproducibility\n",
        "\n",
        "def mix_beta(z, x):\n",
        "    Z1, Z2 = z\n",
        "    if x == 1:\n",
        "        alpha, beta = 25 * Z1 + 1, 25 * (1 - Z1) + 1\n",
        "    else:\n",
        "        alpha, beta = 25 * (1 - Z2) + 1, 25 * Z2 + 1\n",
        "    return alpha, beta\n",
        "\n",
        "# Function to generate dataset\n",
        "def generate_dataset(num_actions, T):\n",
        "    # Step 1: Sample Z(a) for each action (Z1, Z2) independently from Uniform(0, 0.25)\n",
        "    Z = np.random.uniform(0, 0.25, size=(num_actions, 2))\n",
        "\n",
        "    # Step 2: Initialize empty lists for X and mu_infinity\n",
        "    X = np.zeros((num_actions, T))\n",
        "    mu_infinity = np.zeros((num_actions, T))\n",
        "\n",
        "    # Step 3: Generate outcomes and success rates for each time period\n",
        "    Y = np.zeros((num_actions, T))\n",
        "    for t in range(T):\n",
        "        # Step 3.1: Sample binary feature X for each action from Bernoulli(0.5)\n",
        "        X[:, t] = np.random.binomial(1, 0.5, size=num_actions)\n",
        "\n",
        "        # Step 3.2: Sample success rate (mu_infinity) from a mixture of Beta distributions\n",
        "        for i in range(num_actions):\n",
        "            alpha, beta = mix_beta(Z[i], X[i, t])\n",
        "            mu_infinity[i, t] = stats.beta.rvs(alpha, beta)  # Sample from the Beta distribution\n",
        "\n",
        "        # Step 3.3: Generate outcomes Y_t for each action based on mu_infinity\n",
        "        for i in range(num_actions):\n",
        "            Y[i, t] = np.random.binomial(1, mu_infinity[i, t])\n",
        "\n",
        "    return Z, X, mu_infinity, Y\n",
        "\n",
        "# Generate training dataset\n",
        "Z_train, X_train, mu_infinity_train, Y_train = generate_dataset(num_actions_train, T_train)\n",
        "\n",
        "# Generate validation dataset\n",
        "Z_val, X_val, mu_infinity_val, Y_val = generate_dataset(num_actions_val, T_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b3f0826-4622-4c05-960d-2c4833bf95e2",
      "metadata": {
        "id": "8b3f0826-4622-4c05-960d-2c4833bf95e2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a540ddbb-f442-463e-a36c-a980f728ba6b",
      "metadata": {
        "id": "a540ddbb-f442-463e-a36c-a980f728ba6b"
      },
      "outputs": [],
      "source": [
        "# Convert data to PyTorch tensors\n",
        "Z_train_tensor = torch.tensor(Z_train, dtype=torch.float32)\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)  # Add new binary feature X\n",
        "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
        "Z_val_tensor = torch.tensor(Z_val, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).unsqueeze(1)  # Add new binary feature X\n",
        "Y_val_tensor = torch.tensor(Y_val, dtype=torch.float32)\n",
        "\n",
        "# Define the Flexible Neural Network model\n",
        "class FlexibleNN(nn.Module):\n",
        "    def __init__(self, input_dim=23):  # Updated input_dim to include binary feature X\n",
        "        super(FlexibleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 50)\n",
        "        self.fc2 = nn.Linear(50, 50)\n",
        "        self.fc3 = nn.Linear(50, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, Z, X, summary_stat_0, summary_stat_1):\n",
        "        # print(Z.shape, X.shape, summary_stat_0.shape, summary_stat_1.shape)\n",
        "        x = torch.cat((Z, X, summary_stat_0.repeat(1, 5), summary_stat_1.repeat(1, 5)), dim=1)  # Include binary feature X\n",
        "        # print(x.shape)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# Define the Beta-Bernoulli NN model\n",
        "class BetaBernoulliNN(nn.Module):\n",
        "    def __init__(self, input_dim=23):  # Updated input_dim to include binary feature X\n",
        "        super(BetaBernoulliNN, self).__init__()\n",
        "        self.alpha_mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.beta_mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Initialize bias terms to 1 to avoid starting with Beta parameters of value 0\n",
        "        for layer in [self.alpha_mlp[-2], self.beta_mlp[-2]]:\n",
        "            nn.init.constant_(layer.bias, 1.0)\n",
        "\n",
        "    def forward(self, Z, X, summary_stat_0, summary_stat_1):\n",
        "        x = torch.cat((Z, X, summary_stat_0.repeat(1, 5), summary_stat_1.repeat(1, 5)), dim=1)  # Include binary feature X\n",
        "        alpha = self.alpha_mlp(x)\n",
        "        beta = self.beta_mlp(x)\n",
        "        return alpha, beta, alpha / (alpha + beta)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training phase modification with new loss function\n",
        "def train_autoregressive_model(model, train_loader, optimizer, epochs):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)  # Move model to GPU if available\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        running_loss = 0.0\n",
        "        for Z_batch, X_batch, Y_batch in train_loader:\n",
        "            # Move data to GPU if available\n",
        "            Z_batch, X_batch, Y_batch = Z_batch.to(device), X_batch.to(device), Y_batch.to(device)\n",
        "\n",
        "            # Initialize total loss for the batch\n",
        "            tot_loss = 0.0\n",
        "\n",
        "            # Sequentially calculate the loss for each time step t\n",
        "            for t in range(1, Y_batch.shape[1] + 1):\n",
        "                # Get the sequence of previous outcomes up to time t-1\n",
        "                if t == 1:\n",
        "                    previous_outcomes_0 = torch.zeros((Y_batch.shape[0], 1), device=device)  # No previous outcomes at t=1 for X=0\n",
        "                    previous_outcomes_1 = torch.zeros((Y_batch.shape[0], 1), device=device)  # No previous outcomes at t=1 for X=1\n",
        "                else:\n",
        "                    previous_outcomes_0 = Y_batch[:, :t-1] * (1 - X_batch[:, 0, :t-1])\n",
        "                    previous_outcomes_1 = Y_batch[:, :t-1] * X_batch[:, 0, :t-1]\n",
        "\n",
        "                # Create masks to count non-zero entries\n",
        "                mask_0 = (1 - X_batch[:, 0, :t-1]).sum(dim=1, keepdim=True).clamp(min=1)\n",
        "                mask_1 = X_batch[:, 0, :t-1].sum(dim=1, keepdim=True).clamp(min=1)\n",
        "\n",
        "                # Compute summary statistics for X = 0 and X = 1 separately by taking mean over non-zero entries\n",
        "                stat_1_0 = previous_outcomes_0.sum(dim=1, keepdim=True) / mask_0\n",
        "                stat_1_1 = previous_outcomes_1.sum(dim=1, keepdim=True) / mask_1\n",
        "\n",
        "                stat_2_0 = torch.where((1 - X_batch[:, 0, :t-1]).sum(dim=1, keepdim=True) == 0,\n",
        "                                    torch.zeros_like((1 - X_batch[:, 0, :t-1]).sum(dim=1, keepdim=True)),\n",
        "                                    1 / ((1 - X_batch[:, 0, :t-1]).sum(dim=1, keepdim=True) + 1e-9))\n",
        "\n",
        "                stat_2_1 = torch.where(X_batch[:, 0, :t-1].sum(dim=1, keepdim=True) == 0,\n",
        "                                    torch.zeros_like(X_batch[:, 0, :t-1].sum(dim=1, keepdim=True)),\n",
        "                                    1 / (X_batch[:, 0, :t-1].sum(dim=1, keepdim=True) + 1e-9))\n",
        "\n",
        "\n",
        "                # print(stat_1_0.shape, stat_2_0.shape)\n",
        "                summary_stat_t_0 = torch.cat((stat_1_0, stat_2_0), dim=1)\n",
        "                summary_stat_t_1 = torch.cat((stat_1_1, stat_2_1), dim=1)\n",
        "                # print(summary_stat_t_0, summary_stat_t_1)\n",
        "\n",
        "                # print(Z_batch.shape, X_batch[:, :, t-1].shape)\n",
        "\n",
        "                # Forward pass to get model outputs\n",
        "                outputs = model(Z_batch, X_batch[:, :, t-1], summary_stat_t_0, summary_stat_t_1)\n",
        "\n",
        "                # Compute the log-probability for the current time step t\n",
        "                Y_t = Y_batch[:, t-1].unsqueeze(1)\n",
        "                log_probs = Y_t * torch.log(outputs + 1e-9) + (1 - Y_t) * torch.log(1 - outputs + 1e-9)\n",
        "\n",
        "                # Accumulate negative log-likelihood\n",
        "                tot_loss += -torch.sum(log_probs)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            tot_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += tot_loss.item()\n",
        "\n",
        "        # Print loss for every 100 epochs\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            avg_loss = running_loss / len(train_loader)\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    print(\"Training completed.\")\n"
      ],
      "metadata": {
        "id": "0CNjsEuNxjZW"
      },
      "id": "0CNjsEuNxjZW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc82c268-f1dc-4fe0-aef9-63fca4305d96",
      "metadata": {
        "id": "dc82c268-f1dc-4fe0-aef9-63fca4305d96"
      },
      "outputs": [],
      "source": [
        "batch_size = 500          # Batch size for training\n",
        "learning_rate = 0.001     # Learning rate for optimizer\n",
        "epochs = 1000             # Number of epochs for training\n",
        "weight_decay = 0.01       # Weight decay for AdamW optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c34b006-4ebc-4602-8e25-0231ed84f04d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c34b006-4ebc-4602-8e25-0231ed84f04d",
        "outputId": "487cd273-4c37-4855-aca3-6928c2cf9f87"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 100/1000 [06:41<1:00:05,  4.01s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [100/1000], Loss: 101906.6438\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 200/1000 [13:22<53:06,  3.98s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [200/1000], Loss: 101794.5734\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 300/1000 [19:59<46:19,  3.97s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [300/1000], Loss: 101781.1812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 400/1000 [26:34<39:27,  3.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [400/1000], Loss: 101744.4953\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 500/1000 [33:11<33:06,  3.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [500/1000], Loss: 101736.5703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 600/1000 [39:48<26:27,  3.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [600/1000], Loss: 101725.2641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 700/1000 [46:21<19:24,  3.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [700/1000], Loss: 101730.4906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 800/1000 [52:56<13:13,  3.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [800/1000], Loss: 101720.2812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 900/1000 [59:35<06:34,  3.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [900/1000], Loss: 101710.8516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [1:06:14<00:00,  3.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1000/1000], Loss: 101707.3141\n",
            "Training completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "model = FlexibleNN()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "train_dataset = TensorDataset(Z_train_tensor, X_train_tensor, Y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "train_autoregressive_model(model, train_loader, optimizer, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ugnt7eTFEgWZ",
      "metadata": {
        "id": "Ugnt7eTFEgWZ"
      },
      "outputs": [],
      "source": [
        "# Save the model state dictionary after training\n",
        "model_save_path = \"/content/drive/My Drive/contextual_flexible_nn_model.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71b184f9-d892-4c26-a0ad-66a8ad736a42",
      "metadata": {
        "id": "71b184f9-d892-4c26-a0ad-66a8ad736a42"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}